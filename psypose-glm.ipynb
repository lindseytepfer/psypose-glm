{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#brain packages\n",
    "import nibabel as nb\n",
    "import nilearn as nil\n",
    "from nilearn import plotting\n",
    "from nltools import Brain_Data\n",
    "from nltools import Design_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath='/Volumes/Scraplab/data/ds002837/derivatives/'\n",
    "\n",
    "#Generate the subject list and filenames\n",
    "func_data = os.listdir(datapath)\n",
    "sub_ids = [x for x in func_data if ('sub-') in x] #grab all the subject IDs for easy filtering\n",
    "sub_ids.sort() #sort it numerically\n",
    "\n",
    "all_task_subs = [] #net together all the datafiles independent of which task they are from\n",
    "for id in sub_ids:\n",
    "    all_task_subs.append(glob.glob(os.path.join(datapath+id+'/func/*blur_censor_ica.nii.gz'))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video-to-Task Naming Dictionary\n",
    "\n",
    "First, I reconcile the difference between the subject task names and the naming convention of the videos themselves by creating a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys\n",
    "tasknames = ['12yearsaslave','500daysofsummer','backtothefuture','citizenfour',\n",
    "           'littlemisssunshine', 'pulpfiction','split','theprestige',\n",
    "           'theshawshankredemption','theusualsuspects']\n",
    "#values\n",
    "vidnames = ['12_years_a_slave','500_days_of_summer','back_to_the_future','citizenfour',\n",
    "           'little_miss_sunshine', 'pulp_fiction','split','the_prestige',\n",
    "           'the_shawshank_redemption','the_usual_suspects']\n",
    "\n",
    "zippedlist = zip(tasknames,vidnames)\n",
    "tasktovidmap = dict(zippedlist)\n",
    "#task is a key, video is a value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating our Design Matrix\n",
    "\n",
    "In our design matrix, we include body pose annotations extracted from 10 hollywood movies (learn more about the movies and the data set at the [Naturalistic Neuroimaging Database project page](https://www.naturalistic-neuroimaging-database.org/) using a neural network called [PARE](https://github.com/mkocabas/PARE).\n",
    "\n",
    "Using those pose annotations, we are interested in calculating several regressors: person prescence, static synchrony, phasic synchrony, aphasic synchrony.\n",
    "\n",
    "**Person presence** is calculated using the 'frame_ids' value in the PARE output; this tell sus the exact frames where the presence of a person (or several people) was detected on screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Person Tracking Information from PARE\n",
    "\n",
    "For this analysis, we are interested in modeling how the number of people on screen might drive neural activity. Since there can be some cases where there is variation in the number of people are on screen at a time, we take the median across the frames that fall within the 1-second TR range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS BLOCK OF FUNCTIONS WRITTEN BY LANDRY BULLS ###\n",
    "def pkl_to_array(pickle_path, video_path):\n",
    "    data = dict(joblib.load(pickle_path))\n",
    "    n_tracks = len(data)\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frameCount = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    arr = np.zeros((n_tracks, frameCount))\n",
    "    track_ids = list(data.keys())\n",
    "    # track IDs are not always incremental by 1 and are not in order, so I use enumerate here\n",
    "    for t, track in enumerate(track_ids):\n",
    "        # Here, data.get(track) returns the second-level dictionary object for that specific track. I run get() on that\n",
    "        # as well because it is also a dictionary, which will return an array of frame IDs for that track. \n",
    "        frameIDs = data.get(track).get('frame_ids')\n",
    "        # Here I locate the row t (which corresponds to whatever iteration I'm on in my for loop) and I set all\n",
    "        # indices that correspond to frame IDs in that track to 1.\n",
    "        arr[t][frameIDs]=1\n",
    "    return arr\n",
    "\n",
    "def collapse(pickle_array):\n",
    "    frames = pickle_array.shape[1]\n",
    "    regr = np.zeros((frames))\n",
    "    for frame in range(frames):\n",
    "        # Here, we are iterating through each frame (each column at a time) and taking the sum of that column, \n",
    "        # then placing that number at the frame index in our output array. \n",
    "        regr[frame] = sum(pickle_array[:,frame])\n",
    "    return regr\n",
    "\n",
    "def tr_resample(regr_arr, vid_cap, TR, func_length, method='median'):\n",
    "    #all timestamps are in milliseconds\n",
    "    fps = float(vid_cap.get(cv2.CAP_PROP_FPS))\n",
    "    framecount = int(vid_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_timestamps = [(1/fps)*1000*frame for frame in range(framecount)]\n",
    "    brain_timestamps = [TR*1000*tr for tr in range(func_length)]\n",
    "    if method=='mode':\n",
    "        func = lambda x: scipy.stats.mode(x, nan_policy='omit')[0]\n",
    "    elif method=='average':\n",
    "        func = lambda x: np.nanmean(x)\n",
    "    elif method=='min':\n",
    "        func = np.nanmin \n",
    "    elif method=='max':\n",
    "        func = np.nanmax \n",
    "    elif method=='median':\n",
    "        func = np.nanmedian\n",
    "    out = []\n",
    "    def find_nearest(array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx\n",
    "    for stamp in range(func_length):\n",
    "        in_time = brain_timestamps[stamp]\n",
    "        out_time = in_time+1000*TR\n",
    "        in_frame, out_frame = find_nearest(video_timestamps, in_time), find_nearest(video_timestamps, out_time)\n",
    "        regr_frames = regr_arr[in_frame:out_frame]\n",
    "        out.append(func(regr_frames))\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Volumes/Scraplab/data/ds002837/derivatives/'\n",
    "movie_lengths = []\n",
    "for task in tasknames:\n",
    "    task_files = [x for x in all_task_subs if task in x]\n",
    "    task_sub_ids = [x.split(\"/\")[6] for x in task_files if (\"sub\") in x] # this is going to be 9 on Discovery\n",
    "    \n",
    "    for sub in task_sub_ids[0:1]:\n",
    "        task_sub = nb.load(datapath+sub+os.sep+'func'+os.sep+sub+\"_task-\"+task+\"_bold_blur_censor.nii.gz\")\n",
    "        movie_lengths.append(task_sub.shape[3])\n",
    "\n",
    "movie_dict = dict(zip(vidnames, movie_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person presence regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in vidnames:\n",
    "    vidpath = '/Volumes/Scraplab/stimuli/'+video+'.mp4'\n",
    "    pklpath = '/Volumes/Scraplab/data/psypose_outs/psypose_pare_nndb_outs/'+video+'_data/psypose_bodies.pkl'\n",
    "    regr_arr = pkl_to_array(pklpath, vidpath)\n",
    "    regr_arr = collapse(pkl_arr)\n",
    "    vid_cap = cv2.VideoCapture(vidpath)\n",
    "    n_tr = movie_dict[video]\n",
    "    resamp_arr = tr_resample(regr_arr, vid_cap, 1, n_tr)\n",
    "    resamp_df = pd.DataFrame(resamp_arr)\n",
    "    print(\"movie name: \", video, \"resampled array shape:\", resamp_arr.shape, 'resampled df shape:', resamp_df.shape, \"length of TR: \", n_tr)\n",
    "    resamp_df.to_csv(video+\"_regressor.csv\", index=False)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate it out by # of people on screen\n",
    "\n",
    "for vid in vidnames:\n",
    "    df = pd.read_csv('glm_analysis/glm_regressors/'+vid+'_regressor.csv')\n",
    "    df.columns = ['People']\n",
    "    df[[\"Zero\",\"One\",\"Two\",\"Three\",\"Four+\"]] = \"\"\n",
    "    df['Zero'], df['One'], df['Two'], df['Three'], df['Four+'] = np.where(df['People']==0, 1, 0),np.where(df['People']==1, 1, 0), np.where(df['People']==2, 1, 0), np.where(df['People']==3, 1, 0), np.where(df['People']>3, 1, 0)\n",
    "    df.to_csv('glm_analysis/glm_regressors/'+vid+\"_person_presence_dm.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Synchrony Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "static_path = '/Volumes/Scraplab/data/synchrony_vectors/static/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasknames:\n",
    "    video = tasktovidmap[task]\n",
    "    vidpath = '/Volumes/Scraplab/stimuli/'+video+'.mp4'\n",
    "    regr_arr = np.load(static_path+video+\"__synchrony.npy\")\n",
    "    regr_arr = np.nan_to_num(regr_arr)\n",
    "    vid_cap = cv2.VideoCapture(vidpath)\n",
    "    n_tr = movie_dict[video]\n",
    "    resamp_arr = tr_resample(regr_arr, vid_cap, 1, n_tr)\n",
    "    resamp_df = pd.DataFrame(resamp_arr)\n",
    "    print(\"movie name: \", task,video, \"resampled array shape:\", resamp_arr.shape, 'resampled df shape:', resamp_df.shape, \"length of TR: \", n_tr)\n",
    "    resamp_df.to_csv(video+\"_static_sync.csv\", index=False)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netting all the regressors together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasknames:\n",
    "    video = tasktovidmap[task]\n",
    "    vid_regr = pd.read_csv('glm_analysis/glm_regressors/'+video+'_person_presence_dm.csv')\n",
    "    vid_regr.drop('People',axis=1,inplace=True)\n",
    "    sync_regr = pd.read_csv(video+'_static_sync.csv')\n",
    "    regressors = vid_regr.copy()\n",
    "    regressors[\"static\"] = sync_regr[\"0\"]\n",
    "    regressors.to_csv('glm_analysis/glm_regressors/'+video+\"_full_dm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local version\n",
    "\n",
    "TR = 1\n",
    "\n",
    "for task in tasknames[0:1]:\n",
    "    task_files = [x for x in all_task_subs if task in x]\n",
    "    task_sub_ids = [x.split(\"/\")[6] for x in task_files if (\"sub\") in x] # this is going to be 9 on Discovery\n",
    "\n",
    "    for sub in task_sub_ids[0:1]:\n",
    "        #First, load subject's functional data\n",
    "        subj_brain = Brain_Data(glob.glob(os.path.join(datapath+sub+'/func/*blur_censor.nii.gz'))[0])\n",
    "\n",
    "        #Next, because of the naming mis-match, we grab the subject's task name from the first file to map it to the video\n",
    "        video = \"\".join([val for key,val in tasktovidmap.items() if key in subj_brain])\n",
    "\n",
    "        #Now we can load that task video and convolve it\n",
    "        df = pd.read_csv('glm_analysis/glm_regressors/'+video+'_full_dm.csv')\n",
    "        dm = Design_Matrix(df, sampling_freq=1./TR)\n",
    "        dm = dm.convolve()\n",
    "        dm = dm.add_poly(order=0) #this is the intercept, we don't need to add higher order polynomials (eg quadratics etc) because NNDb handled it already\n",
    "\n",
    "        #We ensure that any NaNs in our design matrix are filled, and remove columns with duplicate data \n",
    "        dm_cleaned = dm.clean(verbose=True)\n",
    "\n",
    "        #Set the design matrix, full_dm_cleaned, to the X attribute of the brain data object (subj_run_data)\n",
    "        subj_brain.X = dm_cleaned\n",
    "        stats = subj_brain.regress()\n",
    "        #write our results to a beta map nii file. \n",
    "        stats['beta'].write('glm_analysis/'+sub+'_'+video+'_betamap.nii.gz')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
